---
date: '2025-01-05T17:15:59+08:00'
title: 'MIT: Matformer initialization of Taiwan SLM'
draft: true
cover:
  image: 'img/tinyllama/cover.png'
  alt: 'cover.png'
params:
  math: true
tags: ['TinyLLaMA']
categories: 'Project'
description: 'Paper of Matformer initialization of Taiwan SLM'
summary: "Paper of Matformer initialization of Taiwan SLM"
weight: 10
---

# Abstract
Since the introduction of the Transformer architecture, large language models (LLM) have dominated the NLP field. However, as models scale, inference costs become a key concern. Therefore, small language model emerges. Nonetheless, mainstream small language models are primarily trained on English and simplified Chinese (zh-cn), leaving a relative lack of dedicated models for traditional Chinese (zh-tw).

In this project, we curated a 10B-token Traditional Chinese dataset for continuous pretraining, ensuring quality by sourcing exclusively from trusted web sources. For supervised finetuning, we used 5M dialogue samples generated by Taide to enhance conversational ability. Additionally, we translated existing image-to-text datasets into Chinese to create a multimodal dataset containg 2M turns of dialogue.

Our model starts with a 1.1B-parameter model trained on 3T tokens. We first perform continue-pretraining on 10B tokens, then train two smaller models using the MRL method using the same dataset. To the best of our knowledge, current MRL learning method requires model to train from scratch. In the project, we modify the row and column order of MLP layer weights, allowing direct utilization of the pretrained model without training from scratch, saving enormous computation. We validate our approach on various NLP tasks. Next, we apply supervised finetuning with synthetic data, followed by multimodal finetuning using image-to-text datasets to develop a multimodal Traditional Chinese model. The goal of this project is to contribute to a practical and efficient small language model based on Taiwan cultural background for future applications.

# Motivation
According to the scaling law, larger models generally achieve better performance, but they also come with higher inference costs. While models like GPT-4o, Gemini, and Claude are highly capable, they are not only massive in size but also closed-source. This raises concerns about security and privacy, making it essential for models to run locally on devices like laptops. Using bfloat16, a 7B-parameter model requires around 14GB of VRAM, which is impractical for most consumer GPUs. Therefore, our goal is to train a smaller model that enables more feasible local deployment. The motivation of this study is to establish a small language model specifically tailored for Traditional Chinese and aims to provide a more efficient and applicable solution.

Pretraining is highly resource-intensive. If a single training run could produce models of various sizes, it would significantly reduce computational costs. Inspired by MRL, we reduce the MLP layer weights within Transformer layers, enabling the trained model to support different parameter sizes for greater deployment flexibility. Additionally, we refine the MatFormer approach by carefully initializing parameters, ensuring that reduced-parameter models produce similar outputs to the original, dramatically cutting training time.  

Our research focuses on the following key questions:
Effect of continue-pretraining (cPT). This project aims to investigate whether continue-
pretraining (cPT) can significantly improve the performance of our traditional Chinese lan-
guage models. Through continue-pretraining, we anticipate that the model can better com-
prehend the context of Traditional Chinese, thereby enhancing its performance across down-
stream tasks such as question answering.
Effect of MatFormer initialization.

# Related Work
MRL:
MatFormer:
FlexTron:

# MIT

## Continue-Pretraining

## MatFormer Initialization

## Supervised Finetuning

## Multimodal Finetuning

# Experiments

## Evaluation on base model

## Results on multimodal model

# Conclusion