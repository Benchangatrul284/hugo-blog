---
date: '2025-09-05T17:15:59+08:00'
title: 'Introduction to Large Language Models'
draft: false
cover:
  image: 'img/IntroToLLM/cover.png'
  alt: 'cover'

params:
  math: true
tags: ['LLM']
categories: 'Notes'
description: 'Introduction To LLM'
summary: "Introduction To LLM"
---


# ç›®éŒ„

- [Introduction](#introduction)
    - [Encoding](#encoding)
    - [Generation](#generation)
    - [Decoding](#decoding)
    - [Structure of LLM](#structure-of-llm)
    - [GPT Series](#gpt-series)
- [Transformer Block](#transformer-block)
    - [Self-Attention](#self-attention)
    - [Feed Forward Network (FFN)](#feed-forward-network-ffn)
    - [Residual Connection](#residual-connection)
    - [Layer Normalization](#layer-normalization)
    - [Summary](#summary-of-transformer-block)
- [Self-Attention](#self-attention-1)
    - [Attention Map](#attention-map)
    - [Computation of Attention Map](#computation-of-attention-map)
    - [Summary](#summary-of-self-attention)
- [Positional Embedding](#positional-embedding)
    - [GPT2 Positional Embedding](#gpt2-positional-embedding)
    - [RoPE](#rope)

# Introduction
èªè¨€æ¨¡å‹å’Œå…¶ä»–æ·±åº¦å­¸ç¿’æ¨¡å‹ä¸€æ¨£ï¼Œéƒ½æ˜¯å»å­¸åˆ°ä¸€å€‹ functionï¼Œè®“è¼¸å…¥ç¶“éé€™å€‹ function å¾Œå¯ä»¥å¾—åˆ°æˆ‘å€‘æƒ³è¦çš„è¼¸å‡ºã€‚ä¹Ÿå°±æ˜¯çµ¦å®š x ï¼Œæˆ‘å€‘æœƒå¸Œæœ›å¾—åˆ°ä¸€å€‹ mapping f(x) = y ã€‚
![truncated](/img/IntroToLLM/img1.png)

èªè¨€æ¨¡å‹ä¸»è¦å¯ä»¥åˆ†æˆå…©å¤§éƒ¨åˆ†ï¼Œåˆ†åˆ¥æ˜¯ **Causal Language Model (CLM)** å’Œ **Masked Language Model (MLM)** ã€‚ç¾åœ¨ GPT æ¨¡å‹å¤§å¤šæ˜¯ CLMï¼Œä¹Ÿå°±æ˜¯åˆ©ç”¨æ–‡å­—æ¥é¾çš„æ–¹æ³•ä¾†ç”¢ç”Ÿæ–‡å­—ã€‚è€Œ BERT å‰‡æ˜¯å±¬æ–¼ MLM ï¼Œä¹Ÿå°±æ˜¯çµ¦å®šä¸€æ®µæ–‡å­—å¾Œï¼Œéš¨æ©ŸæŠŠå…¶ä¸­ä¸€äº›å­—é®ä½ (mask)ï¼Œç„¶å¾Œè®“æ¨¡å‹å»é æ¸¬é€™äº›è¢«é®ä½çš„å­—æ˜¯ä»€éº¼ã€‚ é€™ç¯‡ blog post ä¸»è¦è‘—é‡åœ¨ CLM æ¨¡å‹ã€‚

åœ¨ç”Ÿæˆçš„éç¨‹ç•¶ä¸­ï¼Œå…¶å¯¦å¯ä»¥åˆ†æˆä¸‰å€‹éšæ®µ:  
```Tokenizer encoding -> Large language model generation -> Tokenizer decoding```
![truncated](/img/IntroToLLM/img4.png)

åœ¨è©³ç´°ä»‹ç´¹é€™ä¸‰å€‹éšæ®µä¹‹å‰ï¼Œè«‹å®¹è¨±ç­†è€…å²”å€‹é¡Œï¼Œä»‹ç´¹ä¸€ä¸‹ huggingfaceã€‚HuggingFace æ˜¯ä¸€å€‹éå¸¸å¥½ç”¨çš„æ·±åº¦å­¸ç¿’æ¨¡å‹åˆ†äº«å¹³å°ï¼Œè£¡é¢æœ‰éå¸¸å¤šçš„é è¨“ç·´æ¨¡å‹ (pre-trained model)ï¼Œå¯ä»¥è®“æˆ‘å€‘ç›´æ¥æ‹¿ä¾†ä½¿ç”¨ï¼Œè€Œä¸éœ€è¦è‡ªå·±å¾é ­é–‹å§‹è¨“ç·´æ¨¡å‹ã€‚å¦å¤–ï¼ŒHuggingFace ä¹Ÿæä¾›äº†éå¸¸æ–¹ä¾¿çš„ library ï¼Œä¾‹å¦‚ [transformers](https://github.com/huggingface/transformers) ã€‚transformers å¯ä»¥è®“ä½¿ç”¨è€…å¯ä»¥å¿«é€Ÿä¸Šæ‰‹ LLM ï¼Œé€™è£¡èˆ‰ä¸€å€‹ç°¡å–®çš„ä¾‹å­ã€‚
![truncated](/img/IntroToLLM/img8.png)
é€™è£¡å°±æ˜¯åˆ©ç”¨ pipeline é€™å€‹ class ä¾†ä½¿ç”¨ gpt2ï¼Œåªéœ€è¦å¹¾è¡Œ code å°±å¯ä»¥ç”¨ gpt2 æ–‡å­—æ¥é¾ï¼Œéå¸¸æ–¹ä¾¿ã€‚

### Encoding
å› ç‚ºèªè¨€æ¨¡å‹æœ¬èº«çœ‹ä¸æ‡‚æ–‡å­—ã€‚Tokenizer çš„ä¸»è¦åŠŸèƒ½æ˜¯æŠŠæ–‡å­—è½‰æ›æˆæ•¸å­— (token)ï¼Œé€™æ¨£æ¨¡å‹æ‰èƒ½å¤ è™•ç†ã€‚é€™äº› tokenizer é€šå¸¸æœƒæŠŠæ–‡å­—åˆ‡æˆæ¯”è¼ƒå°çš„å–®ä½ï¼Œç„¶å¾Œçµ¦æ¯å€‹å–®ä½ä¸€å€‹å°æ‡‰çš„æ•¸å­— (token id)ã€‚ä¾‹å¦‚ ```"Hello, world!"``` å¯èƒ½æœƒè¢«åˆ‡æˆ ```["Hello", ",", "world", "!"]```ï¼Œç„¶å¾Œå°æ‡‰çš„ token id å¯èƒ½æ˜¯ ```[101, 102, 103, 104]``` (é€™åªæ˜¯èˆ‰ä¾‹ï¼Œå¯¦éš›çš„ token id æœƒæ ¹æ“šä¸åŒçš„ tokenizer è€Œæœ‰æ‰€ä¸åŒ)ã€‚
![truncated](/img/IntroToLLM/img5.png)

### Generation
Large language model çš„ä¸»è¦åŠŸèƒ½æ˜¯æ ¹æ“šè¼¸å…¥çš„ token id ä¾†é æ¸¬ä¸‹ä¸€å€‹ token id çš„æ©Ÿç‡åˆ†ä½ˆã€‚ç¶“éæ¡æ¨£å¾Œï¼Œå°±å¯ä»¥ç”¢ç”Ÿä¸‹ä¸€å€‹ token idã€‚å¦‚ä½•æ¡æ¨£æœƒåœ¨å¾Œé¢ç« ç¯€è¬›è§£ã€‚
![truncated](/img/IntroToLLM/img6.png)

### Decoding
Tokenizer decoding çš„ä¸»è¦åŠŸèƒ½æ˜¯æŠŠæ¨¡å‹è¼¸å‡ºçš„ token id è½‰æ›å›æ–‡å­—ã€‚å¦‚æœæ¨¡å‹è¼¸å‡ºçš„ token id æ˜¯ ```[105]```ï¼Œé‚£éº¼æ ¹æ“š tokenizer çš„å°æ‡‰è¡¨ï¼Œå¯èƒ½æœƒè¢«è½‰æ›å› ```"!"```ã€‚
![truncated](/img/IntroToLLM/img7.png)

### Structure of LLM
LLM æ˜¯ç”±è¨±å¤šå° module çµ„åˆè€Œæˆï¼Œä¸»å¹¹æ˜¯ transformers blocksã€‚embedding table çš„åŠŸèƒ½æ˜¯æŠŠ token id è½‰æ›æˆä¸€å€‹å‘é‡ï¼Œé€šå¸¸å«åš hidden_statesã€‚ç¶“éä¸€å †çš„ transformers blocks å¾Œï¼Œæœ€å¾Œç¶“é lm_head å°‡ hidden_states è½‰æ›æˆ logitsï¼Œå†ç¶“é softmax è½‰æ›æˆæ©Ÿç‡åˆ†ä½ˆã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œlogits çš„ dimension æ˜¯ vocab_size (tokenizer çš„å­—å…¸å¤§å°)ï¼Œä¹Ÿå°±æ˜¯èªªæ¯å€‹ token éƒ½æœƒæœ‰ä¸€å€‹å°æ‡‰çš„æ©Ÿç‡ã€‚
![truncated](/img/IntroToLLM/img9.png)

# GPT Series
OpenAI æ‡‰è©²æ˜¯æ•´æ³¢ LLM çˆ†ç´…çš„é ˜é ­ç¾Šï¼Œç›¸ä¿¡å¤§å®¶å° ChatGPT æ‡‰è©²ä¸é™Œç”Ÿã€‚é€™è£¡ç”¨å¾ˆç°¡å–®çš„ä¸€å¼µåœ– summarize ä¸€ä¸‹ LLM ç™¼å±•çš„æ­·ç¨‹ã€‚
![truncated](/img/IntroToLLM/img10.png)
### Train Time Scaling
å°æ‡‰ä¸Šåœ–å·¦å´ï¼šåœ¨è¨“ç·´æ™‚å †æ›´å¤šè¨ˆç®—è³‡æºèˆ‡è³‡æ–™ã€‚æ—©æœŸ GPT-1/2/3 å¹¾ä¹å°±æ˜¯é€™æ¢è·¯ç·šâ€”â€”æ›´å¤§çš„æ¨¡å‹åƒæ•¸ã€æ›´å¤šçš„æ™‚é–“è¨“ç·´ã€æ›´å¤šè³‡æ–™èˆ‡æ›´å¤š GPUï¼Œç•¶ç„¶é‚„æœ‰æ›´ç‡’éŒ¢ã€‚é€™æ˜¯æ ¹æ“š scaling laws çš„ç¶“é©—æ³•å‰‡ï¼šæ¨¡å‹è¶Šå¤§ã€è³‡æ–™è¶Šå¤šï¼Œè¡¨ç¾è¶Šå¥½ã€‚

### Instruction Following
ä¸Šåœ–ä¸­ç”± train-time scaling èµ°å‘ã€ŒæŒ‡ä»¤è·Ÿéš¨ã€ã€‚æ–¹æ³•é€šå¸¸æ˜¯å…ˆæœ‰ä¸€å€‹å¼·çš„ foundation modelï¼Œå†åšï¼š
- SFTï¼šä»¥å¤§é‡ã€ŒæŒ‡ä»¤ â†’ å›ç­”ã€è³‡æ–™åšç›£ç£å¼å¾®èª¿ï¼Œè®“æ¨¡å‹èƒ½ç†è§£ä¸¦åŸ·è¡ŒæŒ‡ä»¤ï¼›
- RLHF / DPOï¼šç”¨äººé¡åå¥½æˆ– AI åå¥½å°é½Šï¼Œä½¿è¼¸å‡ºæ›´æœ‰å¹«åŠ©ã€æ›´å®‰å…¨ï¼Œæˆ–æ›´åŠ çš„æ”¿æ²»æ­£ç¢ºã€‚
GPT-3.5 å¯è¦–ç‚ºæ­¤éšæ®µçš„ä»£è¡¨ï¼Œå¾æœƒæ–‡å­—æ¥é¾é€²åŒ–åˆ°èƒ½ç†è§£æŒ‡ä»¤ã€‚

### Multimodality
è¼¸å…¥/è¼¸å‡ºä¸å†åªé™æ–‡å­—ï¼Œè€Œæ˜¯åŠ å…¥åœ–ç‰‡ã€éŸ³è¨Šã€å½±ç‰‡ç­‰ã€‚åƒ GPTâ€‘4/4o é€™é¡æ¨¡å‹å³å±¬æ­¤ç¯„ç–‡ã€‚å¾€å¾Œçš„ frontier model å¤§å¤šæœ‰é€™äº›èƒ½åŠ›ã€‚

### Test Time Scaling
å› ç‚ºè¨“ç·´è³‡æ–™ç¼ºä¹ï¼ŒTest Time Scaling è½‰è€Œåœ¨ inference éšæ®µæŠ•å…¥æ›´å¤šè¨ˆç®—ï¼Œä»¥æ›å–æ›´å¥½çš„ç­”æ¡ˆï¼Œè€Œéåªé æ›´å¤§æ¨¡å‹ã€‚æŠ€å·§åŒ…å«ï¼š
OpenAI o series å¯è¦–ç‚ºå¼·åŒ–ã€Œæ¨ç†éç¨‹ã€èˆ‡ã€Œé©—è­‰ã€çš„ä»£è¡¨è·¯ç·šã€‚

### AI Agent
è®“æ¨¡å‹ã€Œèƒ½æ„ŸçŸ¥ã€èƒ½è¦åŠƒã€æœƒè¡Œå‹•ã€ã€‚åƒæ˜¯ MCP å°±æ˜¯ç‚ºäº† AI Agent è€Œç”Ÿçš„ protocolã€‚


# Transformer Block
Transformer Block æ˜¯èªè¨€æ¨¡å‹çš„éª¨å¹¹ã€‚æ¯å€‹ block ç”± Selfâ€‘Attentionã€Feed Forward Networkã€Residual Connection èˆ‡ Layer Normalization çµ„æˆã€‚LLM åŸºæœ¬ä¸Šå°±æ˜¯æŠŠå¾ˆå¤šå€‹ Transformer Blocks ç–Šèµ·ä¾†ã€‚

### Self-Attention
è‡ªæ³¨æ„åŠ›è®“ sequence ä¸­çš„æ¯ä¸€å€‹ token éƒ½èƒ½ã€Œèˆ‡å…¶ä»–æ‰€æœ‰ token å°è©±ã€ã€‚Self-Attention çš„è¼¸å…¥èˆ‡è¼¸å‡ºç¶­åº¦ç›¸åŒï¼Œé€™æ¨£å°±å¯ä»¥æ–¹ä¾¿èˆ‡å…¶ä»–æ¨¡çµ„å †ç–Šã€‚ä¸ç”¨å’Œ CNN ä¸€æ¨£é‚„è¦ç®— feature map çš„ dimensionã€‚
![truncated](/img/IntroToLLM/img11.png)

### Feed Forward Network (FFN)
FFN ä½œç”¨åœ¨ã€Œæ¯å€‹ token å„è‡ªçš„å‘é‡ã€ä¸Šï¼Œå½¼æ­¤ä¸äº’ç›¸æºé€šï¼›å¸¸è¦‹çµæ§‹æ˜¯å…©å±¤çš„ ```nn.Linear``` å’Œ nonlinear functionï¼ˆå¦‚ GELU/SiLUï¼‰ï¼Œä¸­é–“ç¶­åº¦å…ˆæ“´å¼µæˆ ```intermediate size``` å†å£“å› ```hidden size```ï¼Œå› æ­¤æ•´é«”ã€Œè¼¸å…¥ç¶­åº¦ = è¼¸å‡ºç¶­åº¦ã€ã€‚åœ¨å¤šæ•¸ LLM ä¸­ï¼ŒFFN ä½”æ“šäº†ç›¸ç•¶å¤§æ¯”ä¾‹çš„åƒæ•¸é‡èˆ‡è¨ˆç®—é‡ï¼Œå¯ä»¥çœ‹åšæ˜¯æ¨¡å‹å„²å­˜çŸ¥è­˜çš„åœ°æ–¹ã€‚
![truncated](/img/IntroToLLM/img12.png)

### Residual Connection
Residual Connectionæä¾›ä¸€æ¢ä¹¾æ·¨ç›´æ¥çš„ gradient pathï¼Œè®“æ›´æ·±çš„æ¨¡å‹ä¹Ÿèƒ½ç©©å®šæ”¶æ–‚ã€‚å¯¦ä½œä¸Šå°±æ˜¯æŠŠå­æ¨¡çµ„çš„è¼¸å‡ºèˆ‡åŸè¼¸å…¥ç›¸åŠ ï¼ˆaddï¼‰ï¼Œå¸¸è¦‹å¯«æ³•ç‚º `x = x + sublayer(x)`ã€‚
![truncated](/img/IntroToLLM/img13.png)

### Layer Normalization
LayerNorm æœƒæ²¿è‘—è¼¸å…¥çš„ hidden ç¶­åº¦åšæ­£è¦åŒ–ï¼Œç©©å®šç‰¹å¾µå°ºåº¦ä¸¦åŠ é€Ÿè¨“ç·´ã€‚ç‚ºä»€éº¼ç”¨ LayerNormï¼Ÿå› ç‚ºåœ¨ LLM ç•¶ä¸­ï¼Œsequence length æœƒè®Šã€åŒä¸€ batch å…§çš„ token æ•¸ä¹Ÿä¸å›ºå®šï¼ŒBatchNorm çš„çµ±è¨ˆåœ¨æ­¤æƒ…å¢ƒä¸‹ä¸ç©©å®šï¼›ç›¸å°åœ°ï¼Œå› ç‚ºhidden size å›ºå®šï¼Œå› æ­¤ LayerNorm æ›´åˆé©ã€‚
![truncated](/img/IntroToLLM/img14.png)

### Summary of Transformer Block
ç¶œåˆä»¥ä¸Šï¼šæ¯å€‹ Block å¤§è‡´éµå¾ªã€ŒLayerNorm â†’ Selfâ€‘Attention â†’ Residual Connectionã€ä»¥åŠã€ŒLayerNorm â†’ FFN â†’ Residual Connectionã€çš„çµæ§‹ï¼Œåè¦†å †ç–Šå¾Œå°±å½¢æˆäº†æ•´å€‹èªè¨€æ¨¡å‹çš„ä¸»å¹¹ã€‚
![truncated](/img/IntroToLLM/img15.png)


# Self-Attention
Self-Attention æ˜¯ Transformer Block çš„æ ¸å¿ƒã€‚å®ƒè®“ sequence ä¸­çš„æ¯å€‹ token éƒ½èƒ½ã€Œèˆ‡å…¶ä»–æ‰€æœ‰ token å°è©±ã€ï¼Œè§£æ±ºäº† RNN ç¼ºå°‘é•·æœŸè¨˜æ†¶çš„ç—›é»ã€‚å¦å¤–ï¼ŒSelf-Attention åœ¨è¨“ç·´çš„æ™‚å€™å¯ä»¥ parallelizeï¼Œé€Ÿåº¦æ›´å¿«ã€‚ä½†é€™ä¸€åˆ‡æ˜¯æœ‰ä»£åƒ¹çš„ï¼Œå› ç‚ºè¦ä¸€å­—ä¸æ¼è¨˜ä½å‰é¢æ‰€æœ‰çš„ context ï¼Œä¹Ÿé€ å°±äº† LLM åœ¨ inference æ™‚ï¼Œç‚ºäº†è¨˜ä½å‰é¢ token çš„è³‡è¨Šï¼Œæœƒå°è‡´ memory consumption è¶Šä¾†è¶Šå¤§ï¼Œä¹Ÿå°±æ˜¯æƒ¡åæ˜­å½°çš„ KV cache å•é¡Œã€‚
è©±æœ‰äº›æ‰¯é äº†ï¼Œå›åˆ° Self-Attention æœ¬èº«ã€‚
ã€ŒSelfã€ä»£è¡¨åŒä¸€å€‹ sequence ï¼›ã€ŒAttentionã€ä»£è¡¨æ‰¾å‡º token ä¹‹é–“çš„é—œä¿‚ã€‚åˆåœ¨ä¸€èµ·ï¼ŒSelfâ€‘Attention çš„ç›®çš„å°±æ˜¯ä¼°è¨ˆåŒä¸€ sequence ä¸­å„å€‹ token å½¼æ­¤çš„é—œè¯ã€‚æœ¬ç¯€çš„é‡é»æ˜¯ä¸‹é¢å¼å­çš„æ„ç¾©èˆ‡ç›´è¦ºã€‚
![truncated](/img/IntroToLLM/img16.png)

### Attention Map
Attention Map ç”¨ä¾†æè¿° token èˆ‡ token çš„å…©å…©çš„é—œè¯ã€‚å°é•·åº¦ç‚º n çš„è¼¸å…¥ sequence ï¼ŒAttention Map æ˜¯ä¸€å€‹ `n Ã— n` çŸ©é™£ï¼›èˆ‰å€‹ä¾‹å­ï¼Œå¦‚æœåªæœ‰ä¸‰å€‹ tokenï¼Œä¾¿æ˜¯ `3 Ã— 3` çš„ attention mapã€‚çŸ©é™£ä¸­çš„æ¯å€‹å…ƒç´ ç¨±ç‚º attention scoreï¼Œä»£è¡¨ã€Œç¬¬ i å€‹ token å°ç¬¬ j å€‹ token çš„æ³¨æ„åŠ›å¤§å°ã€ã€‚æ³¨æ„é€™å€‹åˆ†æ•¸é€šå¸¸ä¸å°ç¨±ï¼Œä¹Ÿå°±æ˜¯èªª `score(i, j) â‰  score(j, i)`ã€‚åƒæ˜¯çŸ®å“¥å¸ƒæ—çŒ´å“ªèƒ½å¼•èµ·[è—¥å¾‹å·¥ğŸ’ŠğŸŸ¢ğŸ¹](https://www.threads.com/@zelpha_chang)çš„æ³¨æ„åŠ›å‘¢ï¼Œç•¢ç«Ÿè—¥å¾‹å·¥ğŸ’ŠğŸŸ¢ğŸ¹æ˜¯äººä¹‹æ‰€å‘é˜¿ã€‚

ç¶“é per row çš„ Softmax å¾Œï¼Œæ¯ä¸€åˆ—éƒ½æœƒå½¢æˆç¸½å’Œç‚º 1 çš„æ©Ÿç‡åˆ†ä½ˆï¼›åœ¨çœ‹åˆ°æ•´æ®µ input sequence æ™‚ï¼Œæ•´å¼µ Attention Map å¯ä»¥ä¸€æ¬¡è¨ˆç®—å‡ºä¾†ï¼Œä¸æœƒå‘ RNN ä¸€æ¨£åªèƒ½ä¸€æ­¥æ­¥å¾€å‰ç®—é€ æˆæ•ˆç‡ä½è½ã€‚
![truncated](/img/IntroToLLM/img17.png)

### Computation of Attention Map
è¨ˆç®—æ–¹å¼ï¼šå…ˆæŠŠè¼¸å…¥çš„ hidden states ç¶“é WQã€WKã€WV project æˆ Query(Q)ã€Key(K)ã€Value(V)ã€‚attention score ç”± `Q` èˆ‡ `K` çš„ dot product å¾—åˆ°ï¼Œä¹‹å¾Œæ²¿è‘— row æ–¹å‘å¥—ç”¨ `Softmax`ï¼Œä½¿æ¯ä¸€åˆ—çš„ attention score ç¸½å’Œç‚º 1ã€‚
![truncated](/img/IntroToLLM/img18.png)

ç‚ºä»€éº¼è¦é™¤ä»¥æ ¹è™Ÿ d_kï¼Ÿd_k æ˜¯ head dimension ï¼Œå¯ä»¥ç†è§£æˆä¸€å€‹ head å…§çš„ hidden dimensionã€‚è¨ˆç®— `QK^T` å¾Œï¼Œattention score çš„ variance æœƒéš¨ hidden dimension `d_k` å¢å¤§ï¼Œå®¹æ˜“è®“ `Softmax` é£½å’Œã€‚ç‚ºäº†æŠŠ variance ç©©å®šåœ¨æ¥è¿‘ 1ï¼Œæˆ‘å€‘ä»¥ `âˆšd_k` é€²è¡Œ scalingï¼Œä¹Ÿå°±æ˜¯æ¡ç”¨ `QK^T / âˆšd_k`ã€‚
![truncated](/img/IntroToLLM/img19.png)

Combining Valuesï¼šæœ€å¾Œä½¿ç”¨ attention score ä½œç‚º weighted sum çš„æ¬Šé‡ï¼Œå° Value(V) åšåŠ æ¬Šå¹³å‡ï¼Œå¾—åˆ°æ¯å€‹ token æ–°çš„ hidden statesã€‚ç›´è¦ºä¸Šæ˜¯å…ˆæ±ºå®šè©²é—œæ³¨å“ªäº›ä½ç½® (attention map)ã€å„çœ‹å¤šå°‘(value)ï¼Œå†æŠŠè¢«é—œæ³¨ä½ç½®çš„è³‡è¨Šå½™æ•´å›ä¾†ã€‚(weighted sum)
![truncated](/img/IntroToLLM/img20.png)

æœ€å¾Œå¯ä»¥ç”¨é€™å¼µåœ–ä¾†è¡¨ç¤ºå‰é¢çš„æ•¸å­¸å¼å­ã€‚é€™è£¡å‡è¨­ sequence length ç‚º 2ã€‚
![truncated](/img/IntroToLLM/img21.png)

### Summary of Self-Attention
æœ€å¾Œçš„æœ€å¾Œï¼Œå‰é¢çœ‹ä¸æ‡‚éƒ½æ²’æœ‰é—œä¿‚ï¼Œåªè¦è¨˜ä½ä¸€é»ï¼Œè¼¸å‡ºç¶­åº¦ç­‰æ–¼è¼¸å…¥ç¶­åº¦ï¼Œæˆ‘å€‘å¯ä»¥æŠŠå®ƒ self-attention çœ‹æˆä¸€å€‹é»‘ç›’å­ï¼Œè¼¸å…¥ä¸€å€‹ sequenceï¼Œè¼¸å‡ºä¸€å€‹åŒæ¨£é•·åº¦çš„ sequenceï¼Œç„¶å¾Œæ¯å€‹ token éƒ½èƒ½è·Ÿå…¶ä»– token æºé€šå°±å¥½ã€‚
![truncated](/img/IntroToLLM/img22.png)


# Positional Embedding
ç‚ºä»€éº¼éœ€è¦ positional embeddingï¼Ÿå› ç‚º self-attention æœ¬èº«ä¸å¸¶æœ‰ä»»ä½•ä½ç½®è³‡è¨Šã€‚Positional embedding å¯ä»¥ hand craftï¼Œä¹Ÿå¯ä»¥å¾è³‡æ–™ä¸­å­¸ç¿’ã€‚é€™è£¡å°‡ä»‹ç´¹ learned positional embeddingï¼ˆGPT2ï¼‰èˆ‡ rotary positional embeddingï¼ˆRoPEï¼‰ã€‚

### GPT2 Positional Embedding
ä»¥ GPT2 ç‚ºä¾‹ï¼Œ sequence ä¸­æ¯ä¸€å€‹ä½ç½®éƒ½å°æ‡‰ä¸€å€‹ 768 ç¶­çš„å‘é‡ä½œç‚º positional embeddingã€‚GPT2 é è¨­æä¾› 1024 å€‹ä½ç½®ï¼Œå±¬æ–¼ absolute çš„ç·¨ç¢¼æ–¹å¼ï¼Œè€Œä¸”é€™äº›æ¬Šé‡æ˜¯å¾è³‡æ–™ä¸­å­¸ç¿’è€Œä¾†ã€‚
![truncated](/img/IntroToLLM/img23.png)

é€²ä¸€æ­¥è§€å¯Ÿ GPT2 çš„ positional embeddingï¼Œæˆ‘å€‘å¯ä»¥æ‹†è§£åˆ°å–®ä¸€ channel ä¾†çœ‹ï¼›æœƒç™¼ç¾é€™äº›å‘é‡å¤§è‡´å¯è¦–ç‚ºä¸åŒé »ç‡çš„ sine èˆ‡ cosine å‡½æ•¸çš„ç–ŠåŠ ã€‚

![truncated](/img/IntroToLLM/img24.png)


ä¸éï¼ŒGPT2 çš„ positional embedding ä¹Ÿæœ‰å¹¾å€‹ç¼ºé»ï¼šä»–çš„è¡¨ç¾ç›¸ç•¶è¦å¾‹ï¼Œå­˜åœ¨ä¸€å®šç¨‹åº¦çš„ redundancyï¼›æ­¤å¤–ï¼Œå› ç‚ºé å…ˆå®šç¾©å›ºå®šé•·åº¦ï¼Œcontext length æœƒå—åˆ° embedding table å¤§å°çš„é™åˆ¶ã€‚

### RoPE
RoPE æºè‡ª [RoFormer](https://arxiv.org/abs/2104.09864)ï¼Œå…·å‚™è‰¯å¥½çš„ extensibilityï¼Œä¸éœ€è¦äº‹å…ˆå®šç¾©æœ€å¤§çš„ sequence é•·åº¦ã€‚å®ƒé€é rotation matrix å°‡ç›¸å°ä½ç½®ä¿¡æ¯ç·¨å…¥ queries èˆ‡ keysï¼ˆä¸ä½œç”¨æ–¼ values!ï¼‰ã€‚

å…ˆå›é¡§ 2D çš„ rotation matrixã€‚å‡è¨­ hidden dimension ç‚º 2ï¼Œä¸”æˆ‘å€‘æ­£åœ¨è¨ˆç®—ä½ç½® m çš„ positional encodingï¼š

\[
\begin{pmatrix}
\cos(m\theta_{0}) & -\sin(m\theta_{0}) \\
-\sin(m\theta_{0}) & \cos(m\theta_{0})
\end{pmatrix}
\]

æ“´å±•åˆ°æ›´é«˜ç¶­åº¦(d)ï¼š

\[
\begin{pmatrix}
\cos(m\theta_{1}) & -\sin(m\theta_{1}) & 0 & 0 & \cdots & 0 & 0 \\
\sin(m\theta_{1}) &  \cos(m\theta_{1}) & 0 & 0 & \cdots & 0 & 0 \\
0 & 0 & \cos(m\theta_{2}) & -\sin(m\theta_{2}) & \cdots & 0 & 0 \\
0 & 0 & \sin(m\theta_{2}) &  \cos(m\theta_{2}) & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \cdots & \cos(m\theta_{d/2}) & -\sin(m\theta_{d/2}) \\
0 & 0 & 0 & 0 & \cdots & \sin(m\theta_{d/2}) &  \cos(m\theta_{d/2})
\end{pmatrix}
\]

$$
\theta_i=\mathrm{base}^{-\frac{2i}{d}}
$$

å¾—åˆ° rotation matrix å¾Œï¼Œå°‡å®ƒä½œç”¨åœ¨ queries èˆ‡ keys ä¸Šï¼š
![truncated](/img/IntroToLLM/img25.png)


ç”±æ–¼ rotation matrix çµæ§‹ç›¸å°ç¨€ç– (å¾ˆå¤š 0)ï¼Œå¯¦ä½œæ™‚å¸¸ä»¥ hadamard product çš„å½¢å¼èˆ‡ pre-defined çš„å‘é‡é€²è¡Œè¨ˆç®—ã€‚æŸç¨®æ„ç¾©ä¸Šï¼ŒRoPE å¯ä»¥è¦–ç‚ºåœ¨ queriesï¼ˆæˆ– keysï¼‰èˆ‡ä¸€çµ„ pre-defined å‘é‡ä¹‹é–“åš hadamard productã€‚(note: hadamard product ä»£è¡¨ element-wise multiplication)
![truncated](/img/IntroToLLM/img26.png)
